{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d36739ee-a766-4591-9154-1ec49fb3f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0.0 torchvision==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729565be-5bab-45de-b2fb-df4bac1f3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install d2l==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec93e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Molecule Name Molecular Formula  \\\n",
      "0  3-[3-(2,3-dihydroindol-1-yl)propyl]-1-[(4-fluo...        C19H22FN3O   \n",
      "1  3-{[1,1'-biphenyl]-4-ylmethylidene}-6-fluoro-2...        C22H15FO3S   \n",
      "2  1-{[3-(4-methoxyphenyl)-1,2,4-oxadiazol-5-yl]m...        C25H28N4O3   \n",
      "3  3-[2-oxo-2-(4-phenylmethanesulfonylpiperazin-1...       C22H30N4O5S   \n",
      "4  2-{[1,1'-biphenyl]-2-ylamino}-1-(morpholin-4-y...        C18H20N2O2   \n",
      "5  1-(2-{[1,1'-biphenyl]-2-ylamino}acetyl)imidazo...        C17H17N3O2   \n",
      "6  N-(2,3-dihydro-1,4-benzodioxin-6-yl)-2-{[1-(4-...      C23H21N5O3S2   \n",
      "7                                             Tm_322                     \n",
      "8  N-[(1-benzyl-3,5-dimethylpyrazol-4-yl)methyl]-...       C24H25N5O2S   \n",
      "9  N-{[1,1'-biphenyl]-4-yl}-2-(2-oxoquinoxalin-1-...        C22H17N3O2   \n",
      "\n",
      "   METLIN ID Precursor Adduct    CCS1    CCS2    CCS3  CCS_AVG  % CV  \\\n",
      "0  1181481.0    328.1820[M+H]  176.63  176.63  176.63   176.63     0   \n",
      "1  1191359.0    379.0799[M+H]  192.26  192.26  192.26   192.26     0   \n",
      "2  1228206.0    433.2234[M+H]  211.12  211.12  211.12   211.12     0   \n",
      "3  1176932.0    463.2010[M+H]  204.22  204.22  204.22   204.22     0   \n",
      "4  1183857.0    297.1598[M+H]  174.47  174.47  174.47   174.47     0   \n",
      "5  1177044.0    296.1394[M+H]  170.17  170.17  170.17   170.17     0   \n",
      "6  1184006.0    480.1159[M+H]  215.24  215.24  215.24   215.24     0   \n",
      "7        NaN    322.0481[M+H]  154.22  154.22  154.22   154.22     0   \n",
      "8  1205024.0    448.1802[M+H]  198.49  198.49  198.49   198.49     0   \n",
      "9  1184329.0    356.1394[M+H]  195.75  195.75  195.75   195.75     0   \n",
      "\n",
      "        m/z  ... V2205  V2206  V2207 V2208  V2209  V2210  V2211 V2212 V2213  \\\n",
      "0  328.1820  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "1  379.0799  ...   0.0    0.0    0.0   0.0    0.0    0.0    0.0   0.0   1.0   \n",
      "2  433.2234  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "3  463.2010  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "4  297.1598  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "5  296.1394  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "6  480.1159  ...   0.0    0.0    0.0   1.0    0.0    0.0    0.0   0.0   1.0   \n",
      "7  322.0481  ...   NaN    NaN    NaN   NaN    NaN    NaN    NaN   NaN   NaN   \n",
      "8  448.1802  ...   0.0    0.0    0.0   1.0    0.0    0.0    0.0   0.0   1.0   \n",
      "9  356.1394  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "\n",
      "  V2214  \n",
      "0   0.0  \n",
      "1   0.0  \n",
      "2   1.0  \n",
      "3   0.0  \n",
      "4   0.0  \n",
      "5   0.0  \n",
      "6   1.0  \n",
      "7   NaN  \n",
      "8   1.0  \n",
      "9   0.0  \n",
      "\n",
      "[10 rows x 2235 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'METLIN_CCS_vectorfingerprintsVectorized.csv'\n",
    "\n",
    "column_name = 'Dimer.1'\n",
    "value_to_filter = 'Monomer'\n",
    "\n",
    "original_data = pd.read_csv(csv_file_path, nrows = 10)\n",
    "\n",
    "data = original_data[original_data['Dimer.1'] == 'Monomer']\n",
    "\n",
    "# Reset the index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file, overwriting the original file\n",
    "data.to_csv('METLIN_CCS_vectorfingerprintsVectorized_filtered.csv', index=False)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d342aae-c875-435b-bd3d-e00fbfa44d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [0.]]\n",
      "0    [1.0]\n",
      "1    [1.0]\n",
      "2    [1.0]\n",
      "3    [1.0]\n",
      "4    [1.0]\n",
      "5    [1.0]\n",
      "6    [1.0]\n",
      "7    [1.0]\n",
      "8    [1.0]\n",
      "9    [1.0]\n",
      "Name: Adduct, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "adducts = data.iloc[:, 10]\n",
    "adduct_types = ['[M+H]', '[M+Na]', '[2M+H]']\n",
    "encoder = OrdinalEncoder()\n",
    "adducts_2d = np.array(adduct_types).reshape(-1, 1)\n",
    "encoded_adducts = encoder.fit_transform(adducts_2d)\n",
    "print(encoded_adducts)\n",
    "\n",
    "adducts_map = {'[M+H]': encoded_adducts[0], '[M+Na]': encoded_adducts[1], '[2M+H]': encoded_adducts[2]}\n",
    "mapped_adducts = adducts.map(adducts_map)\n",
    "print(mapped_adducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228fe820-2eb2-4e4e-b3f1-620e1a4246b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    176.63\n",
      "1    192.26\n",
      "2    211.12\n",
      "3    204.22\n",
      "4    174.47\n",
      "5    170.17\n",
      "6    215.24\n",
      "7    154.22\n",
      "8    198.49\n",
      "9    195.75\n",
      "Name: CCS_AVG, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "fingerprints = data.iloc[:, 21:121]\n",
    "mz = data.iloc[:, 9]\n",
    "fingerprints.insert(0, 'm/z', mz)\n",
    "X = fingerprints # faltan los adducts \n",
    "y = data.iloc[:, 7]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80b33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Display the name of the active environment\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49de8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([0, 2, 3, 6, 8, 9], dtype='int64') Index([5], dtype='int64') Index([1, 4, 7], dtype='int64')\n",
      "[[418.17368 ]\n",
      " [ 56.181156]]\n"
     ]
    }
   ],
   "source": [
    "from models import SAINT\n",
    "from data_openml import data_prep_openml,task_dset_ids,DataSetCatCon, data_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from augmentations import embed_data_mask\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "categorical_indicator = [False] + [True] * 100 # faltan los adducts\n",
    "categorical_columns = X.columns[1:101].tolist()\n",
    "# categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_dims = [2] * 100 # 2: fingerprints solo pueden tomar dos valores (binario)\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    " \n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(\"category\")\n",
    "\n",
    "X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = [0.65, 0.15, 0.2], size=(X.shape[0],))\n",
    "\n",
    "train_indices = X[X.Set==\"train\"].index\n",
    "valid_indices = X[X.Set==\"valid\"].index\n",
    "test_indices = X[X.Set==\"test\"].index\n",
    "print(train_indices, valid_indices, test_indices)\n",
    "\n",
    "y = pd.DataFrame(y).values\n",
    "\n",
    "X = X.drop(columns=['Set'])\n",
    "\n",
    "nan_mask = X.isna().astype(int)\n",
    "\n",
    "X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "\n",
    "X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "\n",
    "\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32) \n",
    "print(continuous_mean_std)\n",
    "\n",
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,'reg',continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=256, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,'reg', continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=256, shuffle=False,num_workers=4)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,'reg', continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=256, shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442105d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 1) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 47\u001b[0m     valid_rmse \u001b[38;5;241m=\u001b[39m mean_sq_error(model, validloader, device,vision_dset)    \n\u001b[0;32m     48\u001b[0m     test_rmse \u001b[38;5;241m=\u001b[39m mean_sq_error(model, testloader, device,vision_dset)  \n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[EPOCH \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] VALID RMSE: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m     50\u001b[0m         (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, valid_rmse ))\n",
      "File \u001b[1;32m~\\utils.py:111\u001b[0m, in \u001b[0;36mmean_sq_error\u001b[1;34m(model, dloader, device, vision_dset)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_reps \u001b[38;5;241m=\u001b[39m reps[:,\u001b[38;5;241m0\u001b[39m,:]\n\u001b[0;32m    110\u001b[0m     y_outs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmlpfory(y_reps)\n\u001b[1;32m--> 111\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([y_test,y_gts\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mfloat()],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    112\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([y_pred,y_outs],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# import ipdb; ipdb.set_trace() \u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 1) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from utils import count_parameters, classification_scores, mean_sq_error\n",
    "\n",
    "model = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(con_idxs),                \n",
    "    dim = 32, # default                         \n",
    "    dim_out = 1,                       \n",
    "    depth = 6, # default                        \n",
    "    heads = 8, # default                         \n",
    "    attn_dropout = 0.1, # default              \n",
    "    ff_dropout = 0.1, # default                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = 'MLP', # default \n",
    "    attentiontype = 'colrow', # default \n",
    "    final_mlp_style = 'sep', # default\n",
    "    y_dim = 1 # porque es regression \n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "\n",
    "vision_dset = 'store_true'\n",
    "scheduler = 'cosine' # default \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "        _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "        reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "        y_reps = reps[:,0,:]\n",
    "        y_outs = model.mlpfory(y_reps)\n",
    "        loss = criterion(y_outs,y_gts) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         scheduler.step() # ??????????????\n",
    "        running_loss += loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "        test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "        print('[EPOCH %d] VALID RMSE: %.3f' %\n",
    "            (epoch + 1, valid_rmse ))\n",
    "        print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "            (epoch + 1, test_rmse ))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6dc6-86f9-47e8-95be-9bc25311ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2218, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f09155-873b-4858-ba4d-e7b845d96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MyDataModule(d2l.DataModule):\n",
    "    def __init__(self, X, y, p_train=0.7): # training data: 70% of dataset\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        #tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, shuffle=train)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.p_train*len(self.X)) if train else slice(self.p_train*len(self.X), None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "\n",
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X.double(), self.w.double()) + self.b.double()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.lr)\n",
    "\n",
    "\n",
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "my_data = MyDataModule(X, y)\n",
    "model = LinearRegressionScratch(X.shape[1], lr=0.001)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
