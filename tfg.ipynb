{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d36739ee-a766-4591-9154-1ec49fb3f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0.0 torchvision==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729565be-5bab-45de-b2fb-df4bac1f3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install d2l==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec93e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Molecule Name Molecular Formula  \\\n",
      "0   3-[3-(2,3-dihydroindol-1-yl)propyl]-1-[(4-fluo...        C19H22FN3O   \n",
      "1   3-{[1,1'-biphenyl]-4-ylmethylidene}-6-fluoro-2...        C22H15FO3S   \n",
      "2   1-{[3-(4-methoxyphenyl)-1,2,4-oxadiazol-5-yl]m...        C25H28N4O3   \n",
      "3   3-[2-oxo-2-(4-phenylmethanesulfonylpiperazin-1...       C22H30N4O5S   \n",
      "4   2-{[1,1'-biphenyl]-2-ylamino}-1-(morpholin-4-y...        C18H20N2O2   \n",
      "..                                                ...               ...   \n",
      "95  1-[3-(3,4-dimethoxyphenyl)-5-(4-ethoxyphenyl)-...        C21H24N2O4   \n",
      "96  2-(2-methylfuran-3-yl)-5-{[(2-phenyl-1,3-oxazo...       C17H13N3O3S   \n",
      "97  4-{4H,6H,7H-thieno[3,2-c]pyridine-5-sulfonyl}b...      C14H12N2O2S2   \n",
      "98  4-{2,4-dioxo-3-azatricyclo[7.3.1.0^{5,13}]trid...        C21H24N2O3   \n",
      "99  2-{[2-(4-phenylpiperazine-1-carbonyl)phenyl]su...        C24H21N3OS   \n",
      "\n",
      "    METLIN ID Precursor Adduct    CCS1    CCS2    CCS3  CCS_AVG  % CV  \\\n",
      "0   1181481.0    328.1820[M+H]  176.63  176.63  176.63   176.63     0   \n",
      "1   1191359.0    379.0799[M+H]  192.26  192.26  192.26   192.26     0   \n",
      "2   1228206.0    433.2234[M+H]  211.12  211.12  211.12   211.12     0   \n",
      "3   1176932.0    463.2010[M+H]  204.22  204.22  204.22   204.22     0   \n",
      "4   1183857.0    297.1598[M+H]  174.47  174.47  174.47   174.47     0   \n",
      "..        ...              ...     ...     ...     ...      ...   ...   \n",
      "95  1105283.0    369.1809[M+H]  196.76  196.76  196.76   196.76     0   \n",
      "96  1120660.0    340.0750[M+H]  178.12  178.12  178.12   178.12     0   \n",
      "97  1104982.0    305.0413[M+H]  167.71  167.71  167.71   167.71     0   \n",
      "98  1129423.0    353.1860[M+H]  189.91  189.91  189.91   189.91     0   \n",
      "99  1127649.0    400.1478[M+H]  189.10  189.10  189.10   189.10     0   \n",
      "\n",
      "         m/z  ... V2205  V2206  V2207 V2208  V2209  V2210  V2211 V2212 V2213  \\\n",
      "0   328.1820  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "1   379.0799  ...   0.0    0.0    0.0   0.0    0.0    0.0    0.0   0.0   1.0   \n",
      "2   433.2234  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "3   463.2010  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "4   297.1598  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "..       ...  ...   ...    ...    ...   ...    ...    ...    ...   ...   ...   \n",
      "95  369.1809  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "96  340.0750  ...   0.0    0.0    0.0   0.0    0.0    0.0    0.0   0.0   1.0   \n",
      "97  305.0413  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "98  353.1860  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "99  400.1478  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "\n",
      "   V2214  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    1.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "..   ...  \n",
      "95   0.0  \n",
      "96   1.0  \n",
      "97   1.0  \n",
      "98   0.0  \n",
      "99   0.0  \n",
      "\n",
      "[100 rows x 2235 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'METLIN_CCS_vectorfingerprintsVectorized.csv'\n",
    "\n",
    "column_name = 'Dimer.1'\n",
    "value_to_filter = 'Monomer'\n",
    "\n",
    "original_data = pd.read_csv(csv_file_path, nrows = 105)\n",
    "\n",
    "data = original_data[original_data['Dimer.1'] == 'Monomer']\n",
    "\n",
    "# Reset the index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file, overwriting the original file\n",
    "data.to_csv('METLIN_CCS_vectorfingerprintsVectorized_filtered.csv', index=False)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d342aae-c875-435b-bd3d-e00fbfa44d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [0.]]\n",
      "0     [1.0]\n",
      "1     [1.0]\n",
      "2     [1.0]\n",
      "3     [1.0]\n",
      "4     [1.0]\n",
      "      ...  \n",
      "95    [1.0]\n",
      "96    [1.0]\n",
      "97    [1.0]\n",
      "98    [1.0]\n",
      "99    [1.0]\n",
      "Name: Adduct, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "adducts = data.iloc[:, 10]\n",
    "adduct_types = ['[M+H]', '[M+Na]', '[2M+H]']\n",
    "encoder = OrdinalEncoder()\n",
    "adducts_2d = np.array(adduct_types).reshape(-1, 1)\n",
    "encoded_adducts = encoder.fit_transform(adducts_2d)\n",
    "print(encoded_adducts)\n",
    "\n",
    "adducts_map = {'[M+H]': encoded_adducts[0], '[M+Na]': encoded_adducts[1], '[2M+H]': encoded_adducts[2]}\n",
    "mapped_adducts = adducts.map(adducts_map)\n",
    "print(mapped_adducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228fe820-2eb2-4e4e-b3f1-620e1a4246b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     176.63\n",
      "1     192.26\n",
      "2     211.12\n",
      "3     204.22\n",
      "4     174.47\n",
      "       ...  \n",
      "95    196.76\n",
      "96    178.12\n",
      "97    167.71\n",
      "98    189.91\n",
      "99    189.10\n",
      "Name: CCS_AVG, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# adduct = torch.tensor(mapped_adducts)\n",
    "# fingerprints = torch.tensor(data.iloc[:, 21:121].to_numpy(dtype=int))\n",
    "fingerprints = data.iloc[:, 21:121]\n",
    "# fingerprints = torch.tensor(data.iloc[:, 21:2235].to_numpy(dtype=int))\n",
    "# mz = torch.tensor(data.iloc[:, 9].to_numpy(dtype=float))\n",
    "mz = data.iloc[:, 9]\n",
    " \n",
    "# dataset_tensor = X    \n",
    "# X = torch.cat((mz.unsqueeze(1), fingerprints, adduct), dim=1) \n",
    "# X = torch.cat((mz.unsqueeze(1), fingerprints), dim=1)\n",
    "fingerprints.insert(0, 'm/z', mz)\n",
    "X = fingerprints # faltan los adducts \n",
    "\n",
    "# X = data.iloc[:, 21:121]\n",
    "# mz.unsqueeze convierte el vector mz de dimension [100] en una matriz de dimensiones [100,1], para poder concatenarlo\n",
    "\n",
    "# target = y\n",
    "# y = torch.tensor(data.iloc[:, 7].to_numpy(dtype=float))\n",
    "y = data.iloc[:, 7]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80b33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Display the name of the active environment\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8244cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['m/z', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9',\n",
      "       ...\n",
      "       'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100'],\n",
      "      dtype='object', length=101)\n",
      "[False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)\n",
    "categorical_indicator = [False] + [True] * 100\n",
    "print(categorical_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49de8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([ 1,  2,  3,  5,  6,  7,  8,  9, 11, 16, 17, 18, 19, 21, 22, 23, 24, 26,\n",
      "       27, 28, 30, 33, 34, 35, 37, 38, 39, 42, 44, 45, 47, 48, 49, 50, 51, 52,\n",
      "       54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 77,\n",
      "       78, 79, 80, 82, 84, 85, 86, 87, 88, 89, 90, 91, 93, 95, 96, 98],\n",
      "      dtype='int64') Index([10, 20, 25, 29, 32, 36, 40, 41, 46, 81, 94, 97], dtype='int64') Index([0, 4, 12, 13, 14, 15, 31, 43, 53, 56, 69, 72, 74, 75, 76, 83, 92, 99], dtype='int64')\n",
      "[[388.07227]\n",
      " [ 72.74679]]\n"
     ]
    }
   ],
   "source": [
    "from models import SAINT\n",
    "from data_openml import data_prep_openml,task_dset_ids,DataSetCatCon, data_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from augmentations import embed_data_mask\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "categorical_indicator = [False] + [True] * 100 # faltan los adducts\n",
    "categorical_columns = X.columns[1:101].tolist()\n",
    "# categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_dims = [2] * 100 # 2: fingerprints solo pueden tomar dos valores (binario)\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    " \n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(\"category\")\n",
    "\n",
    "X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = [0.65, 0.15, 0.2], size=(X.shape[0],))\n",
    "\n",
    "train_indices = X[X.Set==\"train\"].index\n",
    "valid_indices = X[X.Set==\"valid\"].index\n",
    "test_indices = X[X.Set==\"test\"].index\n",
    "print(train_indices, valid_indices, test_indices)\n",
    "\n",
    "y = pd.DataFrame(y).values\n",
    "\n",
    "X = X.drop(columns=['Set'])\n",
    "\n",
    "nan_mask = X.isna().astype(int)\n",
    "\n",
    "X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "\n",
    "X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "\n",
    "\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32) \n",
    "print(continuous_mean_std)\n",
    "\n",
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,'reg',continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=256, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,'reg', continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=256, shuffle=False,num_workers=4)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,'reg', continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=256, shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "893dbbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "\n",
    "vision_dset = 'store_true'\n",
    "scheduler = 'cosine' # default \n",
    "\n",
    "\n",
    "model = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(con_idxs),                \n",
    "    dim = 32, # default                         \n",
    "    dim_out = 1,                       \n",
    "    depth = 6, # default                        \n",
    "    heads = 8, # default                         \n",
    "    attn_dropout = 0.1, # default              \n",
    "    ff_dropout = 0.1, # default                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = 'MLP', # default \n",
    "    attentiontype = 'colrow', # default \n",
    "    final_mlp_style = 'sep', # default\n",
    "    y_dim = 1 # porque es regression \n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(trainloader, 0):\n",
    "#         optimizer.zero_grad()\n",
    "#         x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "#         _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n",
    "#         reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "#         y_reps = reps[:,0,:]\n",
    "#         y_outs = model.mlpfory(y_reps)\n",
    "#         loss = criterion(y_outs,y_gts) \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "#         running_loss += loss.item()\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "#         test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "#         print('[EPOCH %d] VALID RMSE: %.3f' %\n",
    "#             (epoch + 1, valid_rmse ))\n",
    "#         print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "#             (epoch + 1, test_rmse ))\n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0a1ac05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 73\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
      "[0]\n",
      "[395.46555] [101.06708]\n"
     ]
    }
   ],
   "source": [
    "xxx = X_train['data']\n",
    "z = X_train['mask']\n",
    "print(len(xxx), len(z))\n",
    "\n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "print(cat_idxs)\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "print(con_idxs)\n",
    "X_train['data'][:, con_idxs]\n",
    "np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0)\n",
    "np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "print(train_mean, train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3dc3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       31.3\n",
      "1       86.6\n",
      "2       47.2\n",
      "3       15.4\n",
      "4        8.1\n",
      "        ... \n",
      "1151    11.6\n",
      "1152     2.9\n",
      "1153    10.9\n",
      "1154     0.3\n",
      "1155     5.6\n",
      "Name: counts_for_sons_current_occupation, Length: 1156, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlo\\AppData\\Local\\Temp\\ipykernel_24324\\2569166994.py:2: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  dataset = openml.datasets.get_dataset(541)\n"
     ]
    }
   ],
   "source": [
    "import openml\n",
    "dataset = openml.datasets.get_dataset(541)\n",
    "Xx, yy, categorical_indicator, attribute_names = dataset.get_data(dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "print(yy)\n",
    "# cat_dims, cat_idxs, con_idxs, X_train, y_train, X_valid, y_valid, X_test, y_test, train_mean, train_std = data_prep_openml(541, 5,'reg', datasplit=[.65, .15, .2])\n",
    "# print(cat_dims)\n",
    "# cat_dims2 = [2] * 100\n",
    "# print(cat_dims2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6dc6-86f9-47e8-95be-9bc25311ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2218, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f09155-873b-4858-ba4d-e7b845d96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MyDataModule(d2l.DataModule):\n",
    "    def __init__(self, X, y, p_train=0.7): # training data: 70% of dataset\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        #tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, shuffle=train)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.p_train*len(self.X)) if train else slice(self.p_train*len(self.X), None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "\n",
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X.double(), self.w.double()) + self.b.double()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.lr)\n",
    "\n",
    "\n",
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "my_data = MyDataModule(X, y)\n",
    "model = LinearRegressionScratch(X.shape[1], lr=0.001)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
