{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d36739ee-a766-4591-9154-1ec49fb3f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.0.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: torchvision==0.15.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torch==2.0.0) (3.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (1.23.5)\n",
      "Requirement already satisfied: requests in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from torchvision==0.15.1) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from jinja2->torch==2.0.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from requests->torchvision==0.15.1) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carlo\\anaconda3\\lib\\site-packages (from sympy->torch==2.0.0) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.0.0 torchvision==0.15.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729565be-5bab-45de-b2fb-df4bac1f3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install d2l==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ec93e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Molecule Name Molecular Formula  \\\n",
      "0   3-[3-(2,3-dihydroindol-1-yl)propyl]-1-[(4-fluo...        C19H22FN3O   \n",
      "1   3-{[1,1'-biphenyl]-4-ylmethylidene}-6-fluoro-2...        C22H15FO3S   \n",
      "2   1-{[3-(4-methoxyphenyl)-1,2,4-oxadiazol-5-yl]m...        C25H28N4O3   \n",
      "3   3-[2-oxo-2-(4-phenylmethanesulfonylpiperazin-1...       C22H30N4O5S   \n",
      "4   2-{[1,1'-biphenyl]-2-ylamino}-1-(morpholin-4-y...        C18H20N2O2   \n",
      "..                                                ...               ...   \n",
      "95  1-[3-(3,4-dimethoxyphenyl)-5-(4-ethoxyphenyl)-...        C21H24N2O4   \n",
      "96  2-(2-methylfuran-3-yl)-5-{[(2-phenyl-1,3-oxazo...       C17H13N3O3S   \n",
      "97  4-{4H,6H,7H-thieno[3,2-c]pyridine-5-sulfonyl}b...      C14H12N2O2S2   \n",
      "98  4-{2,4-dioxo-3-azatricyclo[7.3.1.0^{5,13}]trid...        C21H24N2O3   \n",
      "99  2-{[2-(4-phenylpiperazine-1-carbonyl)phenyl]su...        C24H21N3OS   \n",
      "\n",
      "    METLIN ID Precursor Adduct    CCS1    CCS2    CCS3  CCS_AVG  % CV  \\\n",
      "0   1181481.0    328.1820[M+H]  176.63  176.63  176.63   176.63     0   \n",
      "1   1191359.0    379.0799[M+H]  192.26  192.26  192.26   192.26     0   \n",
      "2   1228206.0    433.2234[M+H]  211.12  211.12  211.12   211.12     0   \n",
      "3   1176932.0    463.2010[M+H]  204.22  204.22  204.22   204.22     0   \n",
      "4   1183857.0    297.1598[M+H]  174.47  174.47  174.47   174.47     0   \n",
      "..        ...              ...     ...     ...     ...      ...   ...   \n",
      "95  1105283.0    369.1809[M+H]  196.76  196.76  196.76   196.76     0   \n",
      "96  1120660.0    340.0750[M+H]  178.12  178.12  178.12   178.12     0   \n",
      "97  1104982.0    305.0413[M+H]  167.71  167.71  167.71   167.71     0   \n",
      "98  1129423.0    353.1860[M+H]  189.91  189.91  189.91   189.91     0   \n",
      "99  1127649.0    400.1478[M+H]  189.10  189.10  189.10   189.10     0   \n",
      "\n",
      "         m/z  ... V2205  V2206  V2207 V2208  V2209  V2210  V2211 V2212 V2213  \\\n",
      "0   328.1820  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "1   379.0799  ...   0.0    0.0    0.0   0.0    0.0    0.0    0.0   0.0   1.0   \n",
      "2   433.2234  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "3   463.2010  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "4   297.1598  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "..       ...  ...   ...    ...    ...   ...    ...    ...    ...   ...   ...   \n",
      "95  369.1809  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "96  340.0750  ...   0.0    0.0    0.0   0.0    0.0    0.0    0.0   0.0   1.0   \n",
      "97  305.0413  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "98  353.1860  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "99  400.1478  ...   0.0    0.0    0.0   1.0    1.0    0.0    0.0   0.0   1.0   \n",
      "\n",
      "   V2214  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    1.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "..   ...  \n",
      "95   0.0  \n",
      "96   1.0  \n",
      "97   1.0  \n",
      "98   0.0  \n",
      "99   0.0  \n",
      "\n",
      "[100 rows x 2235 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = 'METLIN_CCS_vectorfingerprintsVectorized.csv'\n",
    "\n",
    "column_name = 'Dimer.1'\n",
    "value_to_filter = 'Monomer'\n",
    "\n",
    "original_data = pd.read_csv(csv_file_path, nrows = 105)\n",
    "\n",
    "data = original_data[original_data['Dimer.1'] == 'Monomer']\n",
    "\n",
    "# Reset the index\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the filtered DataFrame back to the CSV file, overwriting the original file\n",
    "data.to_csv('METLIN_CCS_vectorfingerprintsVectorized_filtered.csv', index=False)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d342aae-c875-435b-bd3d-e00fbfa44d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]\n",
      " [0.]]\n",
      "0     [1.0]\n",
      "1     [1.0]\n",
      "2     [1.0]\n",
      "3     [1.0]\n",
      "4     [1.0]\n",
      "      ...  \n",
      "95    [1.0]\n",
      "96    [1.0]\n",
      "97    [1.0]\n",
      "98    [1.0]\n",
      "99    [1.0]\n",
      "Name: Adduct, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "adducts = data.iloc[:, 10]\n",
    "adduct_types = ['[M+H]', '[M+Na]', '[2M+H]']\n",
    "encoder = OrdinalEncoder()\n",
    "adducts_2d = np.array(adduct_types).reshape(-1, 1)\n",
    "encoded_adducts = encoder.fit_transform(adducts_2d)\n",
    "print(encoded_adducts)\n",
    "\n",
    "adducts_map = {'[M+H]': encoded_adducts[0], '[M+Na]': encoded_adducts[1], '[2M+H]': encoded_adducts[2]}\n",
    "mapped_adducts = adducts.map(adducts_map)\n",
    "print(mapped_adducts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228fe820-2eb2-4e4e-b3f1-620e1a4246b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     176.63\n",
      "1     192.26\n",
      "2     211.12\n",
      "3     204.22\n",
      "4     174.47\n",
      "       ...  \n",
      "95    196.76\n",
      "96    178.12\n",
      "97    167.71\n",
      "98    189.91\n",
      "99    189.10\n",
      "Name: CCS_AVG, Length: 100, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "fingerprints = data.iloc[:, 21:121]\n",
    "mz = data.iloc[:, 9]\n",
    "fingerprints.insert(0, 'm/z', mz)\n",
    "X = fingerprints # faltan los adducts \n",
    "y = data.iloc[:, 7]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80b33fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active environment: base\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Display the name of the active environment\n",
    "print(f\"Active environment: {os.environ['CONDA_DEFAULT_ENV']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49de8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[398.45184]\n",
      " [ 76.58002]]\n",
      "validloader:  <torch.utils.data.dataloader.DataLoader object at 0x00000283FBA30C10>\n"
     ]
    }
   ],
   "source": [
    "from models import SAINT\n",
    "from data_openml import data_prep_openml,task_dset_ids,DataSetCatCon, data_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from augmentations import embed_data_mask\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "categorical_indicator = [False] + [True] * 100 # faltan los adducts\n",
    "categorical_columns = X.columns[1:101].tolist()\n",
    "# categorical_columns = X.columns[list(np.where(np.array(categorical_indicator)==True)[0])].tolist()\n",
    "cont_columns = list(set(X.columns.tolist()) - set(categorical_columns))\n",
    "\n",
    "cat_dims = [2] * 100 # 2: fingerprints solo pueden tomar dos valores (binario)\n",
    "cat_dims = np.append(np.array([1]),np.array(cat_dims)).astype(int) #Appending 1 for CLS token, this is later used to generate embeddings.\n",
    " \n",
    "cat_idxs = list(np.where(np.array(categorical_indicator)==True)[0])\n",
    "con_idxs = list(set(range(len(X.columns))) - set(cat_idxs))\n",
    "\n",
    "for col in categorical_columns:\n",
    "    X[col] = X[col].astype(\"category\")\n",
    "\n",
    "X[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p = [0.65, 0.15, 0.2], size=(X.shape[0],))\n",
    "\n",
    "train_indices = X[X.Set==\"train\"].index\n",
    "valid_indices = X[X.Set==\"valid\"].index\n",
    "test_indices = X[X.Set==\"test\"].index\n",
    "# print(train_indices, valid_indices, test_indices)\n",
    "\n",
    "y = pd.DataFrame(y).values\n",
    "\n",
    "X = X.drop(columns=['Set'])\n",
    "\n",
    "nan_mask = X.isna().astype(int)\n",
    "\n",
    "X_train, y_train = data_split(X,y,nan_mask,train_indices)\n",
    "X_valid, y_valid = data_split(X,y,nan_mask,valid_indices)\n",
    "X_test, y_test = data_split(X,y,nan_mask,test_indices)\n",
    "\n",
    "train_mean, train_std = np.array(X_train['data'][:,con_idxs],dtype=np.float32).mean(0), np.array(X_train['data'][:,con_idxs],dtype=np.float32).std(0)\n",
    "train_std = np.where(train_std < 1e-6, 1e-6, train_std)\n",
    "\n",
    "\n",
    "continuous_mean_std = np.array([train_mean,train_std]).astype(np.float32) \n",
    "print(continuous_mean_std)\n",
    "\n",
    "train_ds = DataSetCatCon(X_train, y_train, cat_idxs,'reg',continuous_mean_std)\n",
    "trainloader = DataLoader(train_ds, batch_size=256, shuffle=True,num_workers=4)\n",
    "\n",
    "valid_ds = DataSetCatCon(X_valid, y_valid, cat_idxs,'reg', continuous_mean_std)\n",
    "validloader = DataLoader(valid_ds, batch_size=256, shuffle=False,num_workers=4)\n",
    "print(\"validloader: \", validloader)\n",
    "\n",
    "test_ds = DataSetCatCon(X_test, y_test, cat_idxs,'reg', continuous_mean_std)\n",
    "testloader = DataLoader(test_ds, batch_size=256, shuffle=False,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f722e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu.\n",
      "tensor([[0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0]])\n",
      "x_categ shape: torch.Size([65, 101])\n",
      "model.categories_offset shape: torch.Size([101])\n",
      "torch.Size([65, 101])\n",
      "Number of embeddings: 201\n",
      "Error: Indices out of range after addition.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_categ\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m num_embeddings \u001b[38;5;129;01mor\u001b[39;00m x_categ\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Indices out of range after addition.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m _ , x_categ_enc, x_cont_enc \u001b[38;5;241m=\u001b[39m embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model, vision_dset)           \n\u001b[0;32m     49\u001b[0m reps \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer(x_categ_enc, x_cont_enc)\n\u001b[0;32m     50\u001b[0m y_reps \u001b[38;5;241m=\u001b[39m reps[:,\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[1;32m~\\augmentations.py:8\u001b[0m, in \u001b[0;36membed_data_mask\u001b[1;34m(x_categ, x_cont, cat_mask, con_mask, model, vision_dset)\u001b[0m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m x_cont\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m      7\u001b[0m x_categ \u001b[38;5;241m=\u001b[39m x_categ \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mcategories_offset\u001b[38;5;241m.\u001b[39mtype_as(x_categ)\n\u001b[1;32m----> 8\u001b[0m x_categ_enc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membeds(x_categ)\n\u001b[0;32m      9\u001b[0m n1,n2 \u001b[38;5;241m=\u001b[39m x_cont\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m _, n3 \u001b[38;5;241m=\u001b[39m x_categ\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from utils import count_parameters, classification_scores, mean_sq_error\n",
    "\n",
    "model = SAINT(\n",
    "    categories = tuple(cat_dims), \n",
    "    num_continuous = len(con_idxs),                \n",
    "    dim = 32, # default                         \n",
    "    dim_out = 1,                       \n",
    "    depth = 6, # default                        \n",
    "    heads = 8, # default                         \n",
    "    attn_dropout = 0.1, # default              \n",
    "    ff_dropout = 0.1, # default                  \n",
    "    mlp_hidden_mults = (4, 2),       \n",
    "    cont_embeddings = 'MLP', # default \n",
    "    attentiontype = 'colrow', # default \n",
    "    final_mlp_style = 'sep', # default\n",
    "    y_dim = 1 # porque es regression \n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device is {device}.\")\n",
    "\n",
    "vision_dset = True\n",
    "scheduler = 'cosine' # default ?¿?¿?¿?¿?¿?¿?\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        x_categ, x_cont, y_gts, cat_mask, con_mask = data[0].to(device), data[1].to(device),data[2].to(device),data[3].to(device),data[4].to(device)\n",
    "\n",
    "        # checkeo de lo que hace embed_data_mask antes del error (todo ok)\n",
    "        print(x_categ)\n",
    "        print(\"x_categ shape:\", x_categ.shape)\n",
    "        print(\"model.categories_offset shape:\", model.categories_offset.shape)\n",
    "        x_categ2 = x_categ + model.categories_offset.type_as(x_categ)\n",
    "        print(x_categ2.shape)\n",
    "        num_embeddings = model.embeds.num_embeddings\n",
    "        print(\"Number of embeddings:\", num_embeddings)\n",
    "        if x_categ.max().item() >= num_embeddings or x_categ.min().item() < 0:\n",
    "            print(\"Error: Indices out of range after addition.\")\n",
    "\n",
    "        _ , x_categ_enc, x_cont_enc = embed_data_mask(x_categ, x_cont, cat_mask, con_mask, model, vision_dset)           \n",
    "        reps = model.transformer(x_categ_enc, x_cont_enc)\n",
    "        y_reps = reps[:,0,:]\n",
    "        y_outs = model.mlpfory(y_reps)\n",
    "        loss = criterion(y_outs,y_gts) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #         scheduler.step() # ??????????????\n",
    "        running_loss += loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_rmse = mean_sq_error(model, validloader, device,vision_dset)    \n",
    "        test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "        print('[EPOCH %d] VALID RMSE: %.3f' %\n",
    "            (epoch + 1, valid_rmse ))\n",
    "        print('[EPOCH %d] TEST RMSE: %.3f' %\n",
    "            (epoch + 1, test_rmse ))\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "741a38ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m valid_rmse \u001b[38;5;241m=\u001b[39m mean_sq_error(model, validloader, device,vision_dset)\n\u001b[1;32m----> 2\u001b[0m test_rmse \u001b[38;5;241m=\u001b[39m mean_sq_error(model, testloader, device,vision_dset)  \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(valid_rmse, test_rmse)\n",
      "File \u001b[1;32m~\\utils.py:107\u001b[0m, in \u001b[0;36mmean_sq_error\u001b[1;34m(model, dloader, device, vision_dset)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dloader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    106\u001b[0m     x_categ, x_cont, y_gts, cat_mask, con_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),data[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),data[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device),data[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 107\u001b[0m     _ , x_categ_enc, x_cont_enc \u001b[38;5;241m=\u001b[39m embed_data_mask(x_categ, x_cont, cat_mask, con_mask,model,vision_dset)           \n\u001b[0;32m    108\u001b[0m     reps \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer(x_categ_enc, x_cont_enc)\n\u001b[0;32m    109\u001b[0m     y_reps \u001b[38;5;241m=\u001b[39m reps[:,\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[1;32m~\\augmentations.py:8\u001b[0m, in \u001b[0;36membed_data_mask\u001b[1;34m(x_categ, x_cont, cat_mask, con_mask, model, vision_dset)\u001b[0m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m x_cont\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m      7\u001b[0m x_categ \u001b[38;5;241m=\u001b[39m x_categ \u001b[38;5;241m+\u001b[39m model\u001b[38;5;241m.\u001b[39mcategories_offset\u001b[38;5;241m.\u001b[39mtype_as(x_categ)\n\u001b[1;32m----> 8\u001b[0m x_categ_enc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membeds(x_categ)\n\u001b[0;32m      9\u001b[0m n1,n2 \u001b[38;5;241m=\u001b[39m x_cont\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m _, n3 \u001b[38;5;241m=\u001b[39m x_categ\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "valid_rmse = mean_sq_error(model, validloader, device,vision_dset) # porque aquí no sale el error??????\n",
    "test_rmse = mean_sq_error(model, testloader, device,vision_dset)  \n",
    "print(valid_rmse, test_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76b6dc6-86f9-47e8-95be-9bc25311ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2218, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "plt.plot(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f09155-873b-4858-ba4d-e7b845d96b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class MyDataModule(d2l.DataModule):\n",
    "    def __init__(self, X, y, p_train=0.7): # training data: 70% of dataset\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "        #tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, shuffle=train)\n",
    "    \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.p_train*len(self.X)) if train else slice(self.p_train*len(self.X), None)\n",
    "        return self.get_tensorloader((self.X, self.y), train, i)\n",
    "\n",
    "\n",
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented from scratch.\"\"\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X.double(), self.w.double()) + self.b.double()\n",
    "\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.lr)\n",
    "\n",
    "\n",
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "\n",
    "\n",
    "my_data = MyDataModule(X, y)\n",
    "model = LinearRegressionScratch(X.shape[1], lr=0.001)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, my_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
